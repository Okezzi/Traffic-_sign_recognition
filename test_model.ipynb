{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras_visualizer import visualizer \n",
    "from matplotlib import pyplot\n",
    "from numpy import expand_dims\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"trafficNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(imagePath):\n",
    "    image = io.imread(imagePath)\n",
    "    image = transform.resize(image, (32, 32))\n",
    "    image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    return image.reshape(1, 32,32,3)\n",
    "\n",
    "def predict_image(image, model):\n",
    "    return np.array(model(image)).round(3)\n",
    "\n",
    "def present_pred(predictions):\n",
    "    label_pred = np.argsort(predictions)[-5:].tolist()\n",
    "    label_pred = label_pred[::-1]\n",
    "    pred_score = [predictions[i] for i in label_pred]\n",
    "    pred = [str(x) for x in label_pred]\n",
    "    return pred, pred_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '16', '1', '2', '11']\n",
      "[0.935, 0.033, 0.031, 0.002, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagePath = \"data/Meta/0.png\"\n",
    "image = prepare_image(imagePath)\n",
    "prediction = predict_image(image, model)\n",
    "p, s = present_pred(predictions=prediction[0])\n",
    "\n",
    "print(p)\n",
    "print(s)\n",
    "plt.bar(p, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 16)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 43)                5547      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 43)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,779\n",
      "Trainable params: 160,555\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d (None, 32, 32, 16)\n",
      "4 conv2d_1 (None, 16, 16, 32)\n",
      "8 conv2d_2 (None, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "# summarize feature map shapes\n",
    "for i in range(len(model.layers)):\n",
    "\tlayer = model.layers[i]\n",
    "\t# check for convolutional layer\n",
    "\tif 'conv' not in layer.name:\n",
    "\t\tcontinue\n",
    "\t# summarize output shape\n",
    "\tprint(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8, 8, 64) dtype=float32 (created by layer 'conv2d_2')>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[8].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer = Model(inputs=model.inputs, outputs=model.layers[8].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = first_layer.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 8, 64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1efc393b4f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(feature_map[0, :, :, 1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 3 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eikpon1\\OneDrive - Louisiana State University\\Documents\\Eloghosa\\Research\\MATH4997\\Traffic_sign_recognition\\test_model.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000008?line=7'>8</a>\u001b[0m \t\tax\u001b[39m.\u001b[39mset_yticks([])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000008?line=8'>9</a>\u001b[0m \t\t\u001b[39m# plot filter channel in grayscale\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000008?line=9'>10</a>\u001b[0m \t\tpyplot\u001b[39m.\u001b[39mimshow(feature_map[\u001b[39m0\u001b[39;49m, :, :, \u001b[39m16\u001b[39;49m], cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000008?line=10'>11</a>\u001b[0m \t\tix \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000008?line=11'>12</a>\u001b[0m \u001b[39m# show the figure\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 3 with size 16"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABoAAAAXCAYAAAAV1F8QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAaUlEQVR4nGP8//8/Az0AE11sGbVo1CJkwEKKYhERkf8KCgo45c+ePfvm////olgl////TzQ2Njb+jw8wMDCcwaV3+MUR3SxiJKWsY2RkfM3AwPAQjxJ5XImBJIsoAcMvjkYtGrVoGFsEAHQgZzhEiX7uAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "square = 32\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "\tfor _ in range(square):\n",
    "\t\t# specify subplot and turn of axis\n",
    "\t\tax = pyplot.subplot(square, square, ix)\n",
    "\t\tax.set_xticks([])\n",
    "\t\tax.set_yticks([])\n",
    "\t\t# plot filter channel in grayscale\n",
    "\t\tpyplot.imshow(feature_map[0, :, :, 16], cmap='gray')\n",
    "\t\tix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eikpon1\\OneDrive - Louisiana State University\\Documents\\Eloghosa\\Research\\MATH4997\\Traffic_sign_recognition\\test_model.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/eikpon1/OneDrive%20-%20Louisiana%20State%20University/Documents/Eloghosa/Research/MATH4997/Traffic_sign_recognition/test_model.ipynb#ch0000010?line=0'>1</a>\u001b[0m visualizer(model, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m'\u001b[39;49m, view\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\Envs\\DLEnv\\lib\\site-packages\\keras_visualizer\\__init__.py:135\u001b[0m, in \u001b[0;36mvisualizer\u001b[1;34m(model, filename, format, view)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/eikpon1/Envs/DLEnv/lib/site-packages/keras_visualizer/__init__.py?line=132'>133</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, hidden_layers_nr):\n\u001b[0;32m    <a href='file:///c%3A/Users/eikpon1/Envs/DLEnv/lib/site-packages/keras_visualizer/__init__.py?line=133'>134</a>\u001b[0m     \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39msubgraph(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcluster_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)) \u001b[39mas\u001b[39;00m c:\n\u001b[1;32m--> <a href='file:///c%3A/Users/eikpon1/Envs/DLEnv/lib/site-packages/keras_visualizer/__init__.py?line=134'>135</a>\u001b[0m         \u001b[39mif\u001b[39;00m layer_types[i] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDense\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/eikpon1/Envs/DLEnv/lib/site-packages/keras_visualizer/__init__.py?line=135'>136</a>\u001b[0m             c\u001b[39m.\u001b[39mattr(color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/eikpon1/Envs/DLEnv/lib/site-packages/keras_visualizer/__init__.py?line=136'>137</a>\u001b[0m             c\u001b[39m.\u001b[39mattr(rank\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "visualizer(model, format='png', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv2d': <keras.layers.convolutional.Conv2D at 0x1c819ae5370>,\n",
       " 'activation': <keras.layers.core.activation.Activation at 0x1c81fc10d00>,\n",
       " 'batch_normalization': <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c81fc56460>,\n",
       " 'max_pooling2d': <keras.layers.pooling.MaxPooling2D at 0x1c81fc951c0>,\n",
       " 'conv2d_1': <keras.layers.convolutional.Conv2D at 0x1c81fc95e80>,\n",
       " 'activation_1': <keras.layers.core.activation.Activation at 0x1c81fcaa100>,\n",
       " 'batch_normalization_1': <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c81fc9bf40>,\n",
       " 'max_pooling2d_1': <keras.layers.pooling.MaxPooling2D at 0x1c81fcb1e50>,\n",
       " 'conv2d_2': <keras.layers.convolutional.Conv2D at 0x1c81fcaac10>,\n",
       " 'activation_2': <keras.layers.core.activation.Activation at 0x1c81fc101f0>,\n",
       " 'batch_normalization_2': <keras.layers.normalization.batch_normalization.BatchNormalization at 0x1c81fcb1a30>,\n",
       " 'max_pooling2d_2': <keras.layers.pooling.MaxPooling2D at 0x1c81fcb8eb0>,\n",
       " 'flatten': <keras.layers.core.flatten.Flatten at 0x1c81fcb6e20>,\n",
       " 'dense': <keras.layers.core.dense.Dense at 0x1c81fcbc820>,\n",
       " 'activation_3': <keras.layers.core.activation.Activation at 0x1c81fcbcd30>,\n",
       " 'dense_1': <keras.layers.core.dense.Dense at 0x1c81fcc2d60>,\n",
       " 'activation_4': <keras.layers.core.activation.Activation at 0x1c81fcd0a00>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4956747ee9fcc6ddeadbf274c4cd0a2bf4908d5263f1ea7b9357de7c41c9c143"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DeepLearningENV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
